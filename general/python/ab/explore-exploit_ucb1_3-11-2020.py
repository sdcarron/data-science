#3-11-2020 AI Reinforcement Learning - Udemy; Explore-Exploit Dilemma; UCB1
#This compares Epsilon-Greedy (Epsilon = .10) to UCB1

#The main idea behind UCB1 algorithm is "confidence bounds" (a mean drawn from 10 samples is less accurate than a mean drawn from 1000 samples)
#If a confidence bound were drawn around each of the above hypothetical means, the bound around the 10-sample mean would be MUCH wider than the bound around the 1000-sample mean


#Chernoff-Hoeffding Bound:
#P{|X_bar - Mu| <= Epsilon} >= 2 * e^(-2 * Epsilon^2 * N)
#Where N is the number of collected samples AND Epsilon is an arbitrary small number
#The above calculation leads to:
#X_UCB_j = X_bar_j + sqrt (2 * (ln (N) / N_j)) *NOTE This is equivalent to choosing an Epsilong equal to the "sqrt (....)" term
#Where N is the total number of times played, and N_j is the number of times Bandit "j" has been played, X_bar_j is the regular sample mean of Bandit "j"


#UCB1 is used basically the same as Optimistic Initial Values (just be greedy, just with respect to X_UCB_j)
#Key ratio is between ln (N) and N_j ... If N_j is small, the UCB will be large; if N_j is large, the UCB will be small;
#ln (N) grows slower than N_j so eventually all UCB will shrink and the algorithm converges to strictly greedy

#The main alteration between UCB1 and Optimistic Initial Values is the argmax () portion of the "greedy" code block section
'''
for n = 1 ... N:
    j = argmax [j'] { bandit [j'].mean + sqrt (2 * (ln (n) / n_j')) }
'''

import numpy as np
import matplotlib.pyplot as plt


class Bandit:
    def __init__ (self, m, mean, n):
        self.m = m
        self.mean = mean
        self.n = n
    
    def pull (self):
        return np.random.randn () + self.m
    
    def update (self, x):
        self.n += 1
        self.mean = (1.0 - (1.0 / self.n)) * self.mean + (1.0 / self.n) * x


def run (m1,m2,m3,n,epsilon):
    n_ucb = np.exp (-10)
    n_greedy = 0
    
    mean_greedy = 0.0
    
    bandits_ucb = [Bandit (m1, m1, n_ucb), Bandit (m2, m2, n_ucb), Bandit (m3, m3, n_ucb)]
    bandits_greedy = [Bandit (m1, mean_greedy, n_greedy), Bandit (m2, mean_greedy, n_greedy), Bandit (m3, mean_greedy, n_greedy)]
    
    data_ucb = np.empty (n)
    data_greedy = np.empty (n)
    
    for i in range (n):
        
        j = np.argmax ([b.mean + np.sqrt (np.log (i) / b.n) for b in bandits_ucb])
        x = bandits_ucb [j].pull ()
        bandits_ucb [j].update (x)
        
        data_ucb [i] = x
    
    average_ucb = np.cumsum (data_ucb) / (np.arange (n) + 1)
    
    
    for i in range (n):
        
        p = np.random.random ()
        
        if p < epsilon:
            j = np.random.choice (3)
        else:
            j = np.argmax ([b.mean for b in bandits_greedy])
        x = bandits_greedy [j].pull ()
        bandits_greedy [j].update (x)
        
        data_greedy [i] = x
    
    average_greedy = np.cumsum (data_greedy) / (np.arange (n) + 1)
    
    
    plt.plot (average_greedy, label="Cumulative Greedy Avg")
    plt.plot (average_ucb, label="Cumulative UCB Avg")
    plt.title ("Cumulative Greedy vs UCB Avg")
    plt.xscale ("log")
    plt.legend ()
    plt.show ()


if __name__ == "__main__":
    
    run (1.0,2.0,3.0,100000,.10)